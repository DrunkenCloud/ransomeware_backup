# Ransomware Analysis and Detection

## Day 1: Foundational Python & Core Cybersecurity Concepts

### What is Programming & Python's Role?

1. Definition of a program, algorithms (step-by-step instructions), and code.
1. Why Python is ideal for cybersecurity: its readability, extensive libraries, and ease of use for scripting, automation, and data analysis.
1. How Python code is executed (the interpreter).
1. Setting up Jupyter Notebooks: Introduction to Jupyter Notebooks as an interactive coding environment. Basic navigation, cell execution, and markdown cells.
1. Introduction to Key Python Libraries:
    * NumPy: The foundational library for numerical computing in Python. Introduction to numpy arrays for efficient storage and manipulation of numerical data. Explain why it's faster for numerical tasks.
    * Pandas: The go-to library for data analysis and manipulation. Introduction to DataFrames as tabular data structures (like spreadsheets). How they organize features and samples.
1. Lab 1: Jupyter & Basic Data Structures
    * Define a list of mock suspicious IP addresses. Practice adding and removing elements using methods like append(), remove().
    * Create a dictionary to represent a simplified malware log entry (e.g., {'timestamp': '...', 'event_type': 'file_write', 'file_path': '...'}). Practice accessing and updating its values (e.g., log_entry['timestamp'] = '...').
    * Demonstrate basic logic: Check if an item is in a list (e.g., '192.168.1.1' in ip_list). Access a value from a dictionary (e.g., log_entry['event_type']).
    * Define a simple function that takes a list of numbers and returns their sum. Call the function.
1. Lab 2: File I/O & Basic Pandas DataFrames
    * Create a simple text file (sample_log.txt) containing mock log entries (e.g., "10:00 AM - Process started: cmd.exe", "10:01 AM - File written: document.txt").
    * Write Python code to read the sample_log.txt file line by line using with open(...) as f: and print its content.
    * Write Python code to write new log entries to another file (new_activity.txt) using f.write().
    * Introduction to Pandas for Tabular Data: Create a small CSV file (e.g., mock_events.csv) with columns like timestamp,event_type,source,destination,bytes_transferred. Populate with a few rows of sample data.
    * Load this CSV into a Pandas DataFrame using pd.read_csv().
    * Practice basic DataFrame operations: displaying the first few rows (.head()), checking data types (.info()), selecting columns (e.g., df['event_type']), and simple filtering (e.g., df[df['event_type'] == 'file_write']). Calculate a simple mean (e.g., df['bytes_transferred'].mean()).

### Foundational Learning

1. What is an Operating System (OS)? The core software that manages computer hardware and software resources (e.g., Windows, Linux, macOS). Key roles in cybersecurity: managing processes, file systems, memory, and user permissions. https://www.youtube.com/watch?v=H4SDPLiUnv4
1. What are Programs? What are Processes: What they are (running programs), how they are created, and the importance of parent-child relationships (e.g., one program launching another). https://www.youtube.com/watch?v=LDhoD4IVElk and https://www.youtube.com/watch?v=7ge7u5VUSbE
1. What is memory? What does it mean when OS is loaded in memory or code. What is stack?
1. File System: How files and folders are organized (directories, paths), and the concept of file permissions (who can read/write/execute).
1. Users & Permissions: Basic concepts of user accounts, administrator privileges, and how malware often tries to escalate privileges.
1. What is a Network? Computers connected to share resources and information. https://www.youtube.com/watch?v=9rABOh8oT24
1. IP Addresses: Unique numerical identifiers for devices on a network (e.g., 192.168.1.1, 10.0.0.5). Brief mention of IPv4 vs. IPv6.
1. Ports: Virtual doorways on a device used by specific applications for communication (e.g., Port 80 for web traffic, Port 443 for secure web traffic).
1. Protocols: Rules for communication (e.g., HTTP for web pages, DNS for resolving domain names to IP addresses, TCP/UDP for data transfer).
1. Client-Server Model: How most network communication works (e.g., your browser (client) requesting a webpage from a web server).
1. OSI Model. TCP/IP Model. https://www.youtube.com/watch?v=Ilk7UXzV_Qc and https://www.youtube.com/watch?v=OTwp3xtd4dg
1. Threats: Definition of malware (malicious software) and its broad categories.
1. Vulnerabilities: Weaknesses in systems that attackers can exploit.
1. Attack Vectors: Common ways malware infects systems (e.g., email attachments, malicious websites, software vulnerabilities).
1. Malware Categories (Brief Overview): Introduce core concepts of Viruses, Worms, Trojans, and then Ransomware. Explain their primary goals. https://www.youtube.com/watch?v=VJFaO2-zsCU
    * Virus: Attaches to legitimate programs, self-replicates when the program is executed, infects other programs.
    * Worm: Self-replicates across networks without human interaction.
    * Trojan: Disguised as legitimate software, but performs malicious actions once executed (e.g., creating a backdoor).
    * Ransomware: Encrypts data and demands payment for decryption.
1. Lab 3: VM Setup & Baselines
    * Download and install VirtualBox or VMware.
    * Create a new VM for Ubuntu/Windows, allocating sufficient RAM, CPU, and disk space.
    * Install the chosen OS.
    * Configure network settings: Initially use NAT for internet access to download updates/tools. Crucially, switch to Host-Only Adapter (or disconnect network) for isolation before any malware analysis to prevent malware from affecting your host machine or the broader internet.
    * Create a clean snapshot immediately after OS installation. This is your first baseline, allowing you to quickly revert to a clean state.
    * Verify basic OS functionality (opening file explorer, command prompt/terminal, system settings).
1. Lab 4: Command Line & Network Investigation
    * File System Navigation: dir (Windows) / ls (Linux) to list files; cd to change directories; mkdir to create directories; del (Windows) / rm (Linux) to delete files.
    * Process Viewing: tasklist (Windows) / ps aux (Linux) to list running processes.
    * Network Configuration: ipconfig (Windows) / ifconfig (Linux) to view IP addresses and network adapter details.
    * Connectivity Test: ping [google.com] to test reachability; nslookup [google.com] to resolve domain names to IP addresses.
    * Discussion: How these fundamental commands are often leveraged by attackers for reconnaissance, persistence, or data manipulation.


### Ethical Framework for Malware Research

1. Legal Boundaries:
    * Never download or distribute live malware without proper authorization
    * Understand local laws regarding malware possession and analysis
    * Institutional Review Board (IRB) considerations for academic research
    * Respecting Terms of Service for analysis platforms
1. Responsible Disclosure:
   * If you discover new vulnerabilities during analysis, follow responsible disclosure practices
   * Coordinate with vendors before public disclosure
   * Consider potential harm vs. security benefit
1. Controlled Environment Principles:
   * Always use isolated, air-gapped systems for analysis
   * Never test malware on systems you don't own
   * Proper disposal of infected VMs and storage media
   * Documentation and chain of custody for evidence
1. Research Ethics:
   * Obtain proper permissions before analyzing organizational data
   * Anonymize sensitive information in reports and publications
   * Consider the dual-use nature of analysis techniques
   * Attribution responsibilities - avoid false accusations

## Day 2: Malware Categories, Static Analysis & Reverse Engineering

1. Ransomware (Recap & Deeper Dive): Unique focus on encryption, ransom notes, and payment mechanisms. Video: https://www.youtube.com/watch?v=S4t2Ao_BcwM&t=744s

### Real-World Ransomware Case Studies

1. Colonial Pipeline Attack (May 2021) - DarkSide Ransomware:
   * Attack Vector: Compromised VPN credentials
   * Impact: 6-day shutdown of major US fuel pipeline
   * Technical Analysis: Double extortion (encryption + data theft)
   * Response: $4.4M ransom payment, FBI recovery of partial funds
   * Lessons: Critical infrastructure vulnerability, importance of network segmentation
2. Kaseya Supply Chain Attack (July 2021) - REvil/Sodinokibi:
   * Attack Vector: Exploited MSP software to reach downstream customers
   * Scale: ~1,500 organizations affected through single vector
   * Technical Analysis: Supply chain compromise, automated deployment
   * Detection Signatures: Specific file modifications, registry changes
   * Lessons: Third-party risk management, supply chain security
3. Costa Rica Government Attack (April 2022) - Conti Ransomware:
   * Attack Vector: Credential theft and lateral movement
   * Impact: Multiple government ministries affected
   * Technical Analysis: Living-off-the-land techniques, AD compromise
   * Geopolitical Impact: State-level incident response
   * Lessons: Nation-state vs. criminal group boundaries

### Properties of Binary Executables

1. Portable Executable (PE) Format (Windows): The blueprint of a Windows program. Key PE Sections:
    * .text: Contains the program's actual code.
    * .data / .rdata: Stores initialized global variables and read-only data.
    * .rsrc: Resources like icons, images, strings embedded in the executable.
    * Import Address Table (IAT): A list of functions the program uses from other system libraries (DLLs). This is critical for static analysis. https://www.youtube.com/watch?v=jf1al_tCxyA
    * Entry Point: The starting instruction address where the OS begins executing the program.
1. API Calls (Application Programming Interface): High-level functions provided by the operating system and its libraries (DLLs like kernel32.dll, advapi32.dll). Programs use these to request services from the OS. Examples: CreateFile (to create or open a file), WriteFile (to write data to a file), RegSetValueEx (to modify a registry key), CreateRemoteThread (to create a thread in another process).
1. Syscalls (System Calls): Direct, low-level requests made by a program to the operating system's kernel. Malware sometimes uses syscalls to bypass security products that only monitor high-level API calls. https://www.youtube.com/watch?v=lhToWeuWWfw
1. Relevance to Analysis: API calls are easier to interpret and reveal a program's intent. Syscalls provide deeper insights but are more complex to monitor and interpret as they require a kernel driver i.e API calls are easier to use and easy to analyze, syscalls are harder to use and harder to analyze.
1. Lab 5: Essential Static Analysis Tools
    * PEStudio: (Windows VM) Powerful tool for analyzing PE files.
    * Sysinternals Suite: (Windows VM) strings.exe (extracts readable strings from files), sigcheck.exe (verifies digital signatures).
    * Linux/Ubuntu Tools: strings command, file command, sha256sum, md5sum, gcc (C compiler), objdump (disassembler).
    * Download and install/copy these tools to your respective VMs. Verify they run correctly. Ensure gcc and objdump are installed in Ubuntu (sudo apt install build-essential binutils).
1. Lab 6: Extracting Strings & Hashes
    * In your Windows VM, use strings.exe on a benign executable (e.g., notepad.exe). Observe the readable strings.
    * In your Linux/Ubuntu VM, use the strings command on a benign executable.
    * Discuss how malicious strings (URLs, file paths, error messages, specific commands) can indicate malware presence.
    * Calculate MD5 and SHA256 hashes of a benign executable using certutil (Windows) or sha256sum/md5sum (Linux).
    * Concept of Hashes as IOCs: Explain that a hash is like a unique "fingerprint" of a file. Malware samples often have known hashes that can be shared as IOCs.
1. Lab 7: Exploring PEStudio https://www.youtube.com/watch?v=Kz7Aw-2sCWI
    * Load a clean, benign executable (e.g., calc.exe, mspaint.exe) into PEStudio.
    * File Information: Identify compiler, linker, build date, and check for digital signatures (explain what a valid signature means and why its absence/invalidity can be suspicious).
    * Sections: Examine the different sections (.text, .data, .rsrc, etc.) and their characteristics (e.g., Executable, Writable). Discuss how unusually named sections or sections with both Executable and Writable permissions can be suspicious (indicates packed or self-modifying code).
    * And many more.

### How to create Binary Executables and Analyse them

1. Introduction to C Programming for Malware Analysis. Why C is relevant: Many operating systems and low-level malware are written in C/C++.
1. Basic C program structure: main() function, header includes (#include).
1. Pointers: The concept of a memory address, how pointers store memory locations, and why malware heavily uses them for direct memory manipulation, bypassing higher-level APIs, and dynamic function calls.
1. Basic C operations relevant to malware: File operations (fopen, fwrite), memory allocation (malloc), string manipulation (strcpy, strcat), bitwise operations (for XORing/encryption).
1. Compilation Process: Source code in C -> Compiler (gcc) -> Object code -> Linker -> Executable file. https://www.youtube.com/watch?v=VDslRumKvRA
1. Disassembly: The process of converting machine code (the raw bytes the CPU executes) back into assembly language (human-readable instructions like MOV, ADD, JMP).
1. Why Disassembly is Key: When source code is unavailable (common for malware), disassembly is the only way to understand its low-level logic. https://www.youtube.com/watch?v=gh2RXE9BIN8
1. Introduction to Assembly Language (Conceptual): Briefly explain registers, memory, and basic instruction types (move data, arithmetic, jumps). https://www.youtube.com/watch?v=4gwYkEK0gOk
1. Obfuscation: Techniques used by malware authors to make their code harder to understand, both for humans and automated analysis tools. String Encryption: Storing strings (e.g., C2 URLs, file names) in an encrypted form and decrypting them at runtime. https://www.youtube.com/watch?v=LfuTMzZke4g
    * Junk Code Insertion: Adding irrelevant instructions to confuse analysts.
    * Control Flow Flattening: Making the program's execution path convoluted.
    * API Hashing/Dynamic Loading: Not importing APIs directly, but resolving their addresses at runtime to hide intentions from static analysis tools like PEStudio.
1. How obfuscation affects disassembly: It can make the assembly code look much more complex, harder to follow, and hide the true purpose of the program.
1. Into the Ghidra, Upload obfuscated and normal code to ghidra (or dogbolt) and see.
1. Lab 8: Writing & Compiling Simple C Code.
    * Open a text editor (e.g., nano or VS Code) in your Ubuntu VM.
    * Write a basic C program (e.g., a "hello world" program, or one that takes a single command-line argument and prints it).
    * Compile the code using gcc -o myprogram myprogram.c.
    * Run the compiled executable (./myprogram).
    * Introduce a "Malicious" Concept (Safe): Modify the C code to include a simple XOR operation on a string, mimicking basic string obfuscation. Compile it again.
1. Lab 9: Reverse Engineering CTF Challenges including stack overflow, biffer overflow etc.
    * Provided CTF Challenges: You will be provided with a few small, compiled C executables. These will mimic simple CTF (Capture The Flag) challenges, designed to be solvable with basic objdump and strings analysis.
    * Example 1: A program that prints a hidden "flag" string if a specific, hardcoded input string is provided as a command-line argument.
    * Example 2: A program that performs a simple arithmetic calculation on an input and prints a result; the goal is to determine the calculation.
    * Example 3: A program with a simple XOR-obfuscated string.

## Day 3: Ransomware Behavior & Analysis

### Analysis of Malware

1. Static vs. Dynamic Analysis.
1. Static Analysis: Examining the code without running it (like reading a book about how a car works). Good for initial triage, quick checks for known patterns (strings, imports). Cannot see runtime obfuscation or conditional behavior. https://youtu.be/KNe4hTVhpPQ?si=kAId21S9ZsK7igDk
1. Dynamic Analysis: Running the malware in a controlled environment and observing its actions (like watching a car being driven and recording its speed, fuel consumption, turns). Essential for understanding true behavior, bypassing obfuscation, and seeing interaction with the OS/network. Disadvantages: Requires isolated environment, time-consuming, may not trigger all code paths. https://youtu.be/i2I37T23mpI?si=fmljFK70aJu0ccZV
1. When to use each approach: They complement each other. Static for quick triage, dynamic for in-depth understanding.
1. Behavioral Characteristics of Malware:
    * File System Activity:
        * Creating, deleting, modifying files.
        * Renaming files (especially with new extensions like .locked, .encrypted).
        * Dropping new executables or configuration files.
        * Modifying system files or critical user documents.
        * Ransomware Specific: Deleting shadow copies (vssadmin.exe delete shadows /all /quiet), dropping ransom notes (.txt, .html, .hta files), mass file encryption.
        * Virus Specific: Attaching to executable files, modifying their content.
    * Network Activity:
        * Connecting to Command and Control (C2) servers (beaconing, sending data, receiving commands).
        * Scanning local or external networks.
        * Downloading additional malicious components.
        * Worm Specific: Spreading over network shares or exploiting vulnerabilities over the network.
    * Registry Activity:
        * Modifying "Run" keys for persistence (malware automatically starts when the system boots).
        * Disabling security features (e.g., Windows Defender, firewall).
        * Storing configuration data.
    * Process Activity:
        * Creating new processes, especially suspicious ones (e.g., cmd.exe launching PowerShell, or a PDF reader launching an executable).
        * Injecting code into legitimate processes (DLL injection).
        * Elevating privileges.
        * Creating mutexes (to ensure only one instance of the malware runs).
        * Self-deletion (removing the original malware file after execution).
**Lab Activity**: Analyze provided IOCs from these cases using the tools learned in previous labs
1. Evasion Techniques:
    * Obfuscation (Runtime): Code that changes itself or dynamically decrypts, making static analysis difficult.
    * Anti-Analysis Techniques: Detecting if it's running in a VM, if a debugger is attached, or if specific analysis tools are present.
    * Time-Delayed Execution: Waiting for a certain time or event before executing its malicious payload.
    * DLL Injection: Injecting malicious code into legitimate processes to hide its activity and bypass security controls.
1. Lab 10: Introducing Cuckoo Sandbox https://www.youtube.com/watch?v=7Nm48OQWmA8
    * Concept: Explain that Cuckoo Sandbox (local) or Any.Run (online) provides a safe, isolated environment where malware can be run without risk to the analyst's machine.
    * Cuckoo Local Setup (Brief): If time permits and internet allows, attempt a very basic Cuckoo setup (install dependencies, start agent). Emphasize the complexity of a full Cuckoo setup and recommend Any.Run for labs.
    * Submitting a Sample: Upload a provided, controlled, and non-destructive malware sample (e.g., a simple Python encryption script that mimics ransomware behavior, or a harmless sample from a public malware repository).
    * Observe Analysis: Watch the interactive analysis session if available, or observe the report generation process.
    * Crucially: Reiterate that this is a controlled environment and students should never execute unknown malware on their host machines.
1. Lab 11: Detailed Sandbox Report Analysis
    * Overview Tab: Identify the malware family (if classified), general behavior summary.
    * Process Graph/Tree: Trace the execution flow, identify parent-child relationships, look for suspicious processes launching others.
    * Files Tab: Examine created, modified, deleted files. Look for ransom notes, encrypted files, new executables, modified system files.
    * Network Tab: Review DNS requests, HTTP/HTTPS connections. Identify C2 IP addresses/domains.
    * Registry Tab: Check for modified registry keys (especially persistence keys).
    * API Calls/Behavioral Log: Review the sequence of API calls. Look for suspicious sequences:
    * File access, then encryption APIs (CryptEncrypt).
    * Calls to vssadmin.exe.
    * Process injection APIs (CreateRemoteThread).
    * Network communication after initial execution.
    * Dropped Files/IOCs: Identify any dropped files (e.g., new executables, ransom notes) and their hashes.
1. Common Feature Types for Malware Detection:
    * File System Features:
        * Count of files created, modified, or deleted within a specific time window.
        * Rate of file writes (writes per second).
        * Presence of specific file extensions (e.g., .exe, .dll, .js, .ps1 created/modified).
        * Ransomware Specific: Count of .encrypted, .locked extensions; presence of "ransom note" strings; count of shadow copy deletions.
    * Registry Features:
        * Count of registry key modifications or creations.
        * Access to sensitive registry paths (e.g., Run keys, security center settings).
    * Process Features:
        * Number of processes created.
        * Depth of the process tree (how many layers of child processes).
        * Specific API call counts (e.g., CreateRemoteThread for injection, VirtualAllocEx for memory allocation, CryptEncrypt for encryption, URLDownloadToFile for downloading).
        * Presence of specific commands executed (e.g., powershell.exe, cmd.exe, vssadmin.exe).
    * Network Features:
        * Number of unique IP addresses/domains contacted.
        * Total bytes sent/received.
        * Unusual port usage.
        * DNS query frequency.
        * New Theory: Visualizing Binaries as Images & Entropy:
        * Binary Files as Images: Explain that any binary file (an executable, a document, an encrypted file) is just a sequence of bytes (0-255). We can map these byte values to grayscale pixel intensities (e.g., 0=black, 255=white), arranging them into a 2D image.
1. Entropy in Images:
    * Entropy: A measure of the randomness or unpredictability of data.
    * Low Entropy: Data that is highly predictable or repetitive (e.g., large blocks of zeros, repetitive code, uncompressed text). When visualized, this will appear structured, with clear patterns or large uniform areas. (Example: A benign, unpacked executable or a text file).
    * High Entropy: Data that is very random-like (e.g., encrypted data, packed/compressed executables, randomized shellcode). When visualized, this will appear noisy, with a lot of varying pixel intensities and no discernible patterns. (Example: A ransomware executable after packing, or an encrypted data file).
    * Why this is a useful feature: Many malware detection systems look for high entropy as a signature of packing or encryption.
1. Lab 12: Generating & Comparing Entropy Images
    * Visual Comparison: Observe and discuss the stark visual difference between the structured low-entropy image and the noisy high-entropy image. Relate this back to packed/encrypted malware.
    * https://binvis.io/#/
1. Basic introduction to Machine Learning
    * Supervised Learning, Unsupervised Learning, and Reinforcement Learning
    * Regression and Classfication
    * Cross-Validation and Feature Selection

## Day 4: Machine Learning

### Introduction to Machine Learning Techniques

1. Machine Learning Paradigms
    * Supervised Learning: Learning from data where we know the "right answer" (the label). Used here for classification (e.g., malware vs. benign).
    * Regression (Introduction): Briefly introduce regression as another type of supervised learning, where the goal is to predict a continuous numerical value (e.g., predicting ransomware family, or severity score). Contrast with classification.
    * Training Data: The examples used to teach the model.
    * Test Data: The unseen examples used to evaluate how well the model learned.
    * Cross-Validation: A technique to more robustly estimate model performance by training and testing on different subsets of the data multiple times. (Briefly explain K-Fold).
    * Bias-Variance Trade-off:
        * Bias: The error from erroneous or overly simplistic assumptions in the learning algorithm. High bias means the model underfits the data (too simple, can't capture complexity).
        * Variance: The error from a model's sensitivity to small fluctuations in the training set. High variance means the model overfits the data (too complex, learns noise, performs poorly on new data).
        * Trade-off: Explain that reducing bias often increases variance, and vice versa. The goal is to find a balance.
1. Common ML Algorithms for Classification:
    * Decision Trees (Recap & Deeper Dive):
        * How they work: They learn a series of "if-then-else" rules from the data to make decisions.
        * Analogy: Like a flowchart.
        * Advantages: Easy to understand and interpret.
    * Random Forest:
        * An Ensemble Method: Combines the predictions of many individual Decision Trees.
        * Bagging (Bootstrap Aggregating): Each tree is trained on a slightly different random subset of the training data.
        * Advantages: More robust, less prone to overfitting than a single Decision Tree, generally performs very well.
    * Support Vector Machines (SVM):
        * How they work: Find the "best" boundary (hyperplane) that separates different classes of data points.
        * Advantages: Effective in high-dimensional spaces, good with clear margin of separation
    * Optional Logistic Regression.
1. Feature Selection (Basics of ML):
    * Why Feature Selection? Discuss the importance of selecting the most relevant features for your model:
    * Reduce Noise: Remove irrelevant or redundant features that can confuse the model.
    * Improve Performance: Models can learn more effectively from a cleaner, more focused set of features.
    * Faster Training: Fewer features mean less data to process, speeding up training.
    * Better Interpretability: Simpler models are easier to understand.
1. Simple Feature Selection Methods:
    * Domain Knowledge: Using your understanding of malware behavior to manually select relevant features (e.g., CryptEncrypt calls for ransomware).
    * Correlation Analysis: Identifying features that are highly correlated with the target variable (label) or with each other (to avoid redundancy).
    * Feature Importance from Tree Models: Explaining how Decision Trees and Random Forests can tell you which features were most useful in making decisions (feature_importances_ attribute).
1. Lab 13: Feature Preprocessing
    * Write the code for simple imputing [[1, 2], [np.nan, 3], [7, 6]]
    * Impute data = [[1.0, 5.0], [2.0, None], [None, 6.0]] using n_neighbors=1 via KNN Imputer. What are the imputed values?
    * Apply MinMaxScaler to [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]
    * Encode using Ordinal Encoder. tshirt_sizes = ['M', 'L', 'S', 'M'] where S < M < L. What are the encoded values?
    * Download the iris.csv data and load it
    * Print the first and last 5 rows of the mentioned dataset
    * How many non null values are there for the dataset feature ‘sepallength’?
    * What is the data type of the feature ‘petalwidth’
    * Plot histograms of sepalwidth, petalwidth, sepallength, petallength and mention the axes
    * Print scatter plots (using seaborn library) with the following configurations for the iris data:
1. Lab 14: Introduction to Regression
    * Apply Linear Regression on the data.
    * Load the data. 
    * Drop the ocean_proximity  categorical feature.
    * How many columns are there with NaN values? Apply SimpleImputer on them with mean strategy.
    * Do a train test split of 70:30 and 80:20 ratio.
    * Apply logistic regression on diabetes.csv, train in 80:20 ratio, draw the logistic regression graph
    * Apply Ridge Regression to predict median house prices using the "Boston Housing Dataset" (Housingdata.csv). The target variable for predictions is MEDV, representing the median house value in $1000s.
1. Lab 15: Introduction to Classifiers
    * Load the Iris and Wine quality datasets. Split the dataset into training and testing sets.
    * Train a Decision Tree classifier and tune hyperparameters. Visualize the decision tree.
    * Evaluate model performance using accuracy, confusion matrix, and classification report
1. Lab 16: Heart Failure Prediction using SVM
    * Build a model to predict 'DEATH_EVENT' using the provided clinical features. Preprocess the data by scaling numerical features with StandardScaler, then perform an 80:20 train-test split. 
    * Train an SVC model, evaluate its performance with a classification report, and use GridSearchCV with specified kernels, C, and gammas to find the best hyperparameters. 
    * Finally, apply SMOTE to address class imbalance and report the resulting accuracy.
1. Lab 17: Epileptic Seizure Prediction: An Overview
    * This assignment involves using PCA and k-NN to predict epileptic seizure events from EEG data. Key steps include:
    * Data Preparation: Load, clean, transform the target variable, and standardize the EEG features.
    * PCA Application: Reduce data dimensionality by applying Principal Component Analysis.
    * Optimal PC Selection: Determine the best number of principal components to use for a k-NN classifier by evaluating f1_score across a range of components.
    * Model Evaluation: Assess the performance of the k-NN model with the optimal PCs using standard classification metrics and a confusion matrix.
1. Lab 18: Simulating Imbalanced Data
    * Create a highly imbalanced version of your mock dataset (e.g., keep 95% benign samples, but only 5% malware samples). This can be done by simply undersampling the malware class or oversampling the benign class from your existing dataset.
    * Train one of your classifiers (e.g., DecisionTreeClassifier) on this new imbalanced dataset.
    * Recalculate and print the accuracy, precision, recall, and F1-score.
    * Observation: Notice how the overall accuracy might still be high, but the recall for the minority class (malware) could significantly drop. This vividly demonstrates why accuracy alone is insufficient for cybersecurity models.

### Machine Learning For CyberSecurity Uses

1. Evaluation Metrics (Crucial for Cybersecurity)
    * The Confusion Matrix: The foundation of classification evaluation. A table that summarizes performance:
    * True Positive (TP): Actual malware correctly identified as malware. (Good!)
    * True Negative (TN): Actual benign correctly identified as benign. (Good!)
    * False Positive (FP): Benign software incorrectly identified as malware (a false alarm). This costs time (investigation) and potentially disrupts legitimate operations.
    * False Negative (FN): Actual malware missed by the detector (an undetected threat). This is often the most dangerous in cybersecurity.
    * Precision: TP / (TP + FP) - "Of all samples predicted as malware, how many were actually malware?" (Focuses on minimizing false alarms).
    * Recall (Sensitivity): TP / (TP + FN) - "Of all actual malware samples, how many did we detect?" (Focuses on minimizing missed threats).
    * F1-Score: The harmonic mean of Precision and Recall. A balanced metric, especially useful when class distribution is uneven.
    * Accuracy: (TP + TN) / (TP + TN + FP + FN) - Overall correct predictions. Why it can be misleading: If 99% of samples are benign, predicting "benign" for everything gives 99% accuracy but catches no malware!
    * ROC Curve & AUC (Area Under the Curve): Visualizes the trade-off between True Positive Rate (Recall) and False Positive Rate across different classification thresholds. AUC summarizes the overall discriminatory power of the model.
1. The problem of imbalanced datasets in cybersecurity: Typically far fewer malware samples than benign samples.
1. Impact: Models can become biased towards the majority class (e.g., always predicting "benign").
1. Solutions (briefly mention): Oversampling the minority class (e.g., duplicating malware samples), undersampling the majority class (e.g., reducing benign samples).
1. Data Sources for Research: Discuss where real-world malware samples and analysis reports are shared for research: VirusTotal, VirusShare, MalwareBazaar, Hybrid-Analysis. (Emphasize ethical use and legal restrictions).
1. Lab 19: Android Ransomware Family Detection
    * This assignment focuses on classifying different families of Android ransomware based on network traffic and behavioral features.
    * Dataset: [Android Ransomware Detection on Kaggle](https://www.kaggle.com/datasets/subhajournal/android-ransomware-detection)
    * Load and preprocess the network monitoring records, which contain various features related to Android ransomware.
    * Train a multi-class classification model to predict the specific Ransomware Type (e.g., SVpeng, PornDroid, Koler, or Benign).
    * Evaluate your model's performance in distinguishing between the different ransomware families and benign traffic using appropriate metrics for multi-class classification (e.g., accuracy, precision, recall, F1-score per class, and a confusion matrix).
1. Lab 20: Windows PE File Ransomware Detection
    * This assignment involves building a model to detect whether a Windows Portable Executable (PE) file is ransomware (malicious) or benign, based on its static characteristics. This is a binary classification problem.
    * Dataset: [ransomware detection dataset](https://www.kaggle.com/datasets/amdj3dax/ransomware-detection-data-set)
    * Load and preprocess the dataset containing features extracted from PE file headers and structures.
    * Train a binary classification model to predict the Benign column (0 for malicious/ransomware, 1 for benign).
    * Evaluate your model's effectiveness in identifying ransomware, reporting metrics such as accuracy, precision, recall, and F1-score for ransomware detection.

## Day 5: Advanced Detection Concepts & Future Challenges

### Neural Network Uses for Ransomeware

1. Brief overview of Neural Networks: The concept of interconnected "neurons" in layers.
1. Why Deep Learning for malware detection: Ability to learn complex, hierarchical patterns directly from large datasets, sometimes with less explicit feature engineering. https://www.youtube.com/watch?v=6M5VXKLf4D4
1. Lab 21: Deep Learning for Heart Failure Prediction
    * This assignment focuses on building and evaluating a deep learning model using a tabular dataset to predict heart failure events.
    * Dataset: [Heart Failure Prediction Dataset](https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction)
    * Data Loading & Preprocessing: Load the dataset. Handle any categorical features (e.g., using One-Hot Encoding). Standardize or normalize numerical features.
    * Deep Learning Model Design: Create a fully connected (dense) neural network architecture suitable for binary classification. Consider the number of layers, neurons per layer, and activation functions (e.g., ReLU for hidden layers, sigmoid for the output).
    * Model Training: Compile your model with an appropriate loss function (e.g., binary cross-entropy) and optimizer. Train the model on your prepared training data.
    * Model Evaluation: Evaluate the trained model's performance on the test set. Report metrics such as accuracy, precision, recall, F1-score, and possibly a confusion matrix.
1. Lab 22: Deep Learning for Malware Classification
    * This assignment focuses on applying deep learning techniques to classify malware based on tabular features extracted from Android applications.
    * Dataset: [Android Malware Dataset for Machine Learning](https://www.kaggle.com/datasets/shashwatwork/android-malware-dataset-for-machine-learning)
    * This dataset contains feature vectors of 215 attributes extracted from Android applications (both malware and benign).
    * Process Data: Load the dataset. Perform any necessary preprocessing steps, such as handling missing values, encoding categorical features (if any), and standardizing or normalizing numerical features.
    * Build & Train Model: Design a fully connected (dense) neural network architecture for binary classification (malware vs. benign). Train the model on your prepared training data.
    * Evaluate: Report classification metrics (accuracy, precision, recall, F1-score, and a confusion matrix) on the test set.
1. Convolutional Neural Networks (CNNs) for Image-Based Detection:
    * CNNs for Images (Conceptual): Introduce CNNs as a type of neural network particularly well-suited for image processing tasks. Focus on the idea that they can automatically learn visual patterns from data like our "entropy images" without needing explicit human-designed features.
    * How they work (Simplified): Briefly explain that they use "filters" to detect simple patterns (like edges or textures) and combine them to recognize more complex patterns.
    * How CNNs could classify "entropy images" (like those generated in Lab 3.5) as benign, packed, or encrypted by learning visual patterns in the raw pixel data.
1. Lab 23: MNIST Number Classification with CNNs
    * This assignment focuses on building and evaluating a Convolutional Neural Network (CNN) for classifying handwritten digits from the MNIST dataset.
    * Data Preparation: Load the MNIST dataset, normalize pixel values, and reshape the data for CNN input.
    * CNN Model Definition: Design a simple CNN architecture (e.g., using Keras or PyTorch) with convolutional, pooling, and dense layers.
    * Model Training: Compile and train the CNN model on the prepared MNIST training data.
    * Model Evaluation: Evaluate the trained model's performance on the test set, reporting key metrics like accuracy and loss.
1. Lab 24: Malware Entropy Image Classification with CNNs
    * This assignment challenges you to build a CNN model to classify malware based on their visual representations derived from binary entropy.
    * Dataset: [Malware Detection with Images](https://www.kaggle.com/datasets/kokykg/malware-detection-with-images)
    * Load and preprocess the malware image dataset (e.g., normalization, resizing).
    * Design and train a CNN architecture suitable for image classification.
    * Evaluate the model's performance in classifying different malware families, reporting accuracy and other relevant metrics.

### Teachniques to improve Machine Learning Training and Running

1. Ensemble Learning (Deeper Dive):
    * Voting Classifier: Simple ensemble where multiple diverse models make predictions, and the final prediction is based on a "majority vote."
    * Boosting (Concept): Iteratively training models, with each new model focusing on correcting the errors of the previous ones (e.g., AdaBoost - conceptually explain how it weights misclassified samples).
    * Why ensembles are powerful: They reduce variance and bias, leading to more stable and accurate predictions.
    * Incremental Learning (Online Learning)
1. Batch Training vs. Incremental Learning:
    * Batch: Training the model once on a fixed dataset. If new data arrives, you retrain the whole model.
    * Incremental (Online): The model learns continuously from new data streams without needing to retrain from scratch. It "updates" its knowledge.
1. Why it's important for malware detection: New malware variants appear constantly, requiring models to adapt without downtime.
Concept of partial_fit() in scikit-learn.
1. Lab 25: Implementing a Voting Classifier
    * Import VotingClassifier from sklearn.ensemble.
    * Initialize your individual models (from Day 4).
    * Create a VotingClassifier instance, providing a list of your models.
    * Train the VotingClassifier on your training data.
    * Make predictions and calculate evaluation metrics (Precision, Recall, F1, AUC).
    * Comparison: Compare the performance of the Voting Classifier against the best individual model. Discuss if the ensemble provides an improvement and why.
1. Lab 26: Basic Feature Selection & Visualization
    * Train a RandomForestClassifier and access its feature_importances_ attribute.
    * Feature Importance Plot: Use matplotlib.pyplot.bar to create a bar chart showing the importance of each feature. Discuss which features the model found most relevant for classification.
    * Feature Distribution Plots:
        * Choose 2-3 of the most important features.
        * Use seaborn.histplot or matplotlib.pyplot.hist to plot the distribution of these features separately for 'benign' and 'malware' samples. Observe if there's a clear separation.
        * Use seaborn.scatterplot or matplotlib.pyplot.scatter to create a scatter plot of two important features, coloring points by their 'label' (benign/malware). Observe if the malware and benign samples cluster differently in this 2D feature space.
        * Discussion: Relate these visualizations back to the concepts of feature selection and how well different features separate the classes.
1. Real-Time Detection Architectures can have Live Detection or DElayed by a Sandbox, Distributed or Centralized.
1. Live Detection vs. Sandbox Delay:
    * Live/Endpoint Detection: Antivirus, EDR (Endpoint Detection and Response) tools that monitor systems in real-time. Benefits: immediate response. Challenges: resource usage, false positives, evasion.
    * Sandbox Analysis: Delayed, in-depth analysis. Benefits: detailed insights, safe environment. Challenges: time delay, may not catch all behaviors if malware detects sandbox.
1. Centralized vs. Decentralized Systems:
    * Centralized (e.g., SIEM - Security Information and Event Management): Collects logs from many sources into one place for correlation and analysis.
    * Decentralized (e.g., EDR agents): Detection logic runs directly on individual endpoints.
1. Hybrid Malware Detection Approach
    * Neural Networks for Feature Extraction: Neural nets learn powerful, automated features (embeddings) from raw system data to represent potential malware characteristics.
    * Classical ML for Final Classification: These learned features are then fed into traditional machine learning models, like decision trees, for the ultimate malware detection decision.
    * Comparative Performance Analysis: The combined approach's effectiveness will be measured against other malware detection methods to find the best solution.
    * In a distributed Hybrid system as such, by sharing only model weights updates between models, it procides GDPR compliance.
1. Briefly mention Federated Learning (Concept): A future trend where models are trained collaboratively on decentralized datasets (e.g., on different organizations' endpoints) without raw data ever leaving the local environment, preserving privacy while improving global model intelligence.
1. Limitations & Future Frontiers:
    * Dataset Limitations & Biases: The difficulty of obtaining perfectly representative and balanced datasets for malware research. Models can only learn what they're shown.
    * Malware Evolution: Polymorphism (malware constantly changes its code), novel attack vectors (e.g., supply chain attacks, living off the land techniques using legitimate tools), and zero-day exploits make detection a moving target.
    * Adversarial Attacks on ML Models (Intro): How attackers can deliberately craft malware to evade ML-based detectors by subtly modifying features or inputs.### Threat Intelligence & IOC Sharing Ecosystem
1. Types of Threat Intelligence:
   * Strategic: High-level trends, threat actor profiles
   * Tactical: TTPs (Tactics, Techniques, Procedures), campaign analysis
   * Operational: Specific attacks, attribution, timing
   * Technical: IOCs, signatures, YARA rules
2. IOC Categories & Formats:
   * File Hashes (MD5, SHA1, SHA256)
   * Network Indicators (IPs, domains, URLs)
   * Registry Keys and File Paths
   * YARA Rules for pattern matching
   * STIX/TAXII standards for structured sharing
3. Threat Intelligence Platforms:
   * VirusTotal: Community-driven malware analysis
   * MISP (Malware Information Sharing Platform): Open-source TI sharing
   * Commercial Feeds: Recorded Future, CrowdStrike, FireEye
   * Government Sources: US-CERT, NCSC advisories
4. Information Sharing Organizations:
   * ISACs (Information Sharing and Analysis Centers)
   * FIRST (Forum of Incident Response and Security Teams)
   * Private sector consortiums and trust groups

### Integration with Existing Security Systems

1. The Role of SIEM: Explain what a SIEM is (a platform that collects, aggregates, and correlates security logs and events from various sources).
1. Lab 27: Simulating Incremental Learning
    * Choose a model that supports partial_fit (e.g., SGDClassifier or PassiveAggressiveClassifier).
    * Split your dataset into multiple "chunks" representing new data arriving over time.
    * Train the model initially on the first chunk.
    * Then, in a loop, use partial_fit() to update the model with each subsequent chunk of data.
    * After each partial_fit(), evaluate the model's performance on a fixed test set.
    * Observation: Observe how the model's performance might change as it learns from new data incrementally. Discuss the benefits (continuous adaptation) and challenges (potential for concept drift if new data changes drastically).
    * Feeding ML Insights to SIEM: How the outputs of our ML models (e.g., a "malware detected" alert, a confidence score, or even raw feature sets) can be formatted (e.g., as Syslog messages, JSON, or via APIs) and sent to a SIEM.
    * Benefits of SIEM Integration: Allows security analysts to see ML-generated alerts alongside other security events, providing a holistic view. Enables correlation of ML alerts with network traffic, authentication logs, etc., for richer context and faster incident response. Facilitates automated actions based on combined alerts.
1. Legacy System Support: Discuss the challenges of integrating detection with older systems (e.g., Windows XP, legacy industrial control systems).
    * Agent-based (if possible): Deploying lightweight agents if the legacy OS supports it like OSSEC.
    * Agentless/Network-based Monitoring: Monitoring network traffic going to/from legacy systems, analyzing external logs if available, or using network intrusion detection systems (NIDS) to observe behavior without touching the endpoint like OSSEC.
    * Log Forwarding: Configuring legacy systems to forward any available security logs (even basic ones) to a central log collector or SIEM.
    * Proxying/Gateway: Placing a modern system as a proxy or gateway in front of a legacy system to inspect traffic or logs.
    * Key Considerations: Data format compatibility, network bandwidth, processing power on legacy systems, compliance requirements
1. GDPR Compliance:
    * Protect Personal Data: Ransomware targets personal data. GDPR mandates its protection through robust security measures.
    * Security by Design: Implement encryption, MFA, access controls, and regular vulnerability assessments as core security practices to meet GDPR.
    * Ransomware = Data Breach: A ransomware attack is considered a personal data breach under GDPR if it impacts personal data.
    * Breach Notification:
        * DPA: Notify the relevant Data Protection Authority (DPA) within 72 hours of awareness, unless risk is low.
        * Individuals: Notify affected individuals without undue delay if the breach poses a high risk to their rights.
    * Incident Response: Have a clear plan for containing, investigating, and mitigating ransomware, including fulfilling notification duties.
    * Accountability: Maintain thorough documentation of security measures and incident responses to demonstrate compliance.
    * Penalties: Be aware of significant fines (up to €20M or 4% of global turnover) for GDPR non-compliance.
1. DPDP Act, 2023 Compliance:
    * Applicability: Applies to digital personal data processing in India, and to entities processing Indian residents' data for offering goods/services in India.
    * Data Protection: Implement "reasonable security safeguards" like encryption and access controls to prevent data breaches as required by DPDP.
    * Data Principal Rights: Respect individual rights to access, correct, and erase their data.
    * Mandatory Breach Notification:
        * DPB: Notify the Data Protection Board of India (DPB) "as soon as practicable" (e.g., within 72 hours) upon discovery of a breach.
        * Individuals: Inform affected individuals if the breach risks harm.
    * Incident Management: Establish clear protocols for detecting, mitigating, and reporting data breaches efficiently.
    * Penalties: Understand the strict fines (up to ₹250 crore) for DPDP Act violations, including failure to prevent or report breaches.
